{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to import some libraries to use, as well as moving the working directory to the folder containing the raw data .csv files: In my case this is ../Swallows/ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "indent": "0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "path_root=\"/Users/domansa/Documents/BiodiversityAnalysis/Data/NBN_downloads/Swallows/\"\n",
    "os.chdir(path_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the maps We also need to grab the outline of the UK for the map: \n",
    "the easiest way to do this is using geopandas' 'naturalearth_lowres' basemap:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import geopandas\n",
    "import geoplot\n",
    "\n",
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads from NBN top out at 500k observations, so we need to first recombine all the downloaded .csv files into a single file, called 'combinedSwallows.csv':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "indent": "0"
   },
   "outputs": [],
   "source": [
    "# list all .csv files in the folder:\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "all_filenames.sort(key=str.lower) # sort by name, not strictly necessary\n",
    "\n",
    "# load all files in list, then concatenate into a single pandas datagrame \n",
    "combined_csv = pd.concat([pd.read_csv(f,low_memory=False) for f in all_filenames ])\n",
    "\n",
    "# export as one concatenated csv file\n",
    "combined_csv.to_csv( \"combinedSwallows.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data annotations offered by NBN are very rich - most of this we don't need a this point.\n",
    "Let's grab just the key information and copy it into a new Pandas dataframe):\n",
    " - dates, locations for the sightings \n",
    "\n",
    "To compare days of the year, we also need to recast the day/month timestamps into a single number from 1~366. We'll add this as a new column in the dataframe.\n",
    "We also want to know when they are arriving departing with weekly resolution, so lets add this too:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "indent": "-20"
   },
   "outputs": [],
   "source": [
    "Swallows= combined_csv[[\"eventDate processed\",\n",
    "                        \"year processed\",\n",
    "                        \"month processed\",\n",
    "                        \"day processed\",\n",
    "                        \"decimalLatitude processed\",\n",
    "                        \"decimalLongitude processed\"]]\n",
    "\n",
    "# Get the day of the year\n",
    "# NB this will jump a bit for leap years! @TODO: fix this annotation\n",
    "Swallows[\"day_of_year\"] = pd.to_datetime(Swallows[\"eventDate processed\"]).dt.dayofyear\n",
    "\n",
    "# Get the week of the year\n",
    "Swallows[\"week\"]        = pd.to_datetime(Swallows[\"eventDate processed\"]).dt.week\n",
    "\n",
    "# Take a peek at the first few entries:\n",
    "Swallows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a sanity check and take a quick look to see the spatial distribution of the data. We'll use Panda's overloaded plotting functionality for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "indent": "-20"
   },
   "outputs": [],
   "source": [
    "# Quick and dirty plotting and binning of the data points\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "\n",
    "ha1 = Swallows.plot.scatter(x='decimalLongitude processed',\n",
    "                            y='decimalLatitude processed',ax=axs[0],s = 0.1)\n",
    "axs[0].axis('off')\n",
    "\n",
    "ha2 = Swallows.plot.hexbin(x='decimalLongitude processed',\n",
    "                            y='decimalLatitude processed',ax=axs[1],gridsize=20)\n",
    "axs[1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by plotting some animations of the data through time. \n",
    "There are more elegant tools for this but this is easiest to get working.\n",
    "We will create one low resolution figure for each day, looping over years and each day\n",
    "Each day's plot is saved to an output folder and will become one movie frame.\n",
    "\n",
    "N.B. We need to make a new folder before running each of the blocks of code below:\n",
    "- one called /plots \n",
    "- one called /heatmap\n",
    "\n",
    "This can be done programatically, but we'll allow the user to stay in control this time.\n",
    "\n",
    "Run the cell below, and then check that a series of .png plots has appeared in the /plots folder.\n",
    "To turn the individual pictures into a .GIF, we'll use imagemagick - download this first before the following line will work.\n",
    "\n",
    "Now, ouside python, open a terminal (bash, Windows Powershell etc.), navigate to the output directory using ```cd``` and run the following:\n",
    "\n",
    "```convert -delay 50 -dispose previous birdDist*.png animated_gapminder.gif```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "indent": "-20"
   },
   "outputs": [],
   "source": [
    "# %%capture --no-display # Hide the warning outputs\n",
    "\n",
    "# scatter the report data, loop over years and days for each plot, plot each bird sighting by coordinates\n",
    "\n",
    "# Outer loop is years, inner loop is days.\n",
    "# We'll only plot a restricted time range of 2015~2021, to get all included date ranges, you can use this instead\n",
    "# for counter, year in enumerate(sorted(list(pd.unique(Swallows['year processed'])))):\n",
    "#     for day_counter, enumerate(sorted(list(pd.unique(Swallows['day_of_year'])))):\n",
    "        \n",
    "for counter, year in enumerate(np.arange(2015,2021)):      \n",
    "    for day_counter, day in enumerate(np.arange(1,365)):   \n",
    "        # index out the relevant data:         \n",
    "        data_sub = Swallows[(Swallows['year processed'] == year) & (Swallows['day_of_year'] == day)]\n",
    "\n",
    "        # skip this day if there's no sighting data:\n",
    "        if data_sub.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        # Plot the outline of the UK using the underlying shapely geometry from the 'world' mapping object\n",
    "        ax = world[world.name == 'United Kingdom'].plot(color='white', edgecolor='black')  \n",
    "\n",
    "        # Convert the Lat and Long info to a geopandas dataframe, using the 'EPSG:4326' coordinate ref system (crs)\n",
    "        gdf = geopandas.GeoDataFrame(data_sub, \n",
    "                                     geometry=geopandas.points_from_xy(\n",
    "                                        data_sub['decimalLongitude processed'], \n",
    "                                        data_sub['decimalLatitude processed']),\n",
    "                                    crs='EPSG:4326')\n",
    "        \n",
    "        # Scatter plot the bird sightings, make them transparent-ish with a bit of 'alpha' \n",
    "        gdf.plot(ax=ax, alpha=0.2, markersize=5, c='b')\n",
    "        \n",
    "        # Stop the map jumping around by fixing the lat and long corners of the plot:\n",
    "        plt.xlim([-8, 3])\n",
    "        plt.ylim([49, 61])\n",
    "        \n",
    "        # Use the date as the title\n",
    "        plt.title('%s' % (pd.unique(data_sub['eventDate processed'])[0]))        \n",
    "\n",
    "        # Hide the bounding box\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Save this figure as a new figure at 100dpi resolution\n",
    "        plt.savefig(path_root + 'plot/birdDist_%s_%d.png' % (year, day), dpi=100)\n",
    "        plt.close() # Optionally remove the plot to stop the accumulating in the output below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the same approach but using a different plotting command to get a smoothed heatmap for each plot instead of a scatterplot. These will be saved to the /heatmap folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "indent": "-40",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%capture --no-display # Hide the warning outputs\n",
    "\n",
    "# This code block is as above, but uses a smoothed heatmap 'kernel density estimation' approach to summarise the data\n",
    "\n",
    "for counter, year in enumerate(np.arange(2015,2021)):\n",
    "    for day_counter, day in enumerate(np.arange(1,365)):\n",
    "        data_sub = Swallows[(Swallows['year processed'] == year) & (Swallows['day_of_year'] == day)]\n",
    "\n",
    "        if data_sub.shape[0] <10: # The KDE struggles with fewer than ten observations, so trip low count days\n",
    "            continue\n",
    "            \n",
    "        ax = world[world.name == 'United Kingdom'].plot(color='white', edgecolor='black')  # the frame is turned off            \n",
    "\n",
    "        gdf = geopandas.GeoDataFrame(data_sub, \n",
    "                                     geometry=geopandas.points_from_xy(\n",
    "                                        data_sub['decimalLongitude processed'], \n",
    "                                        data_sub['decimalLatitude processed']),\n",
    "                                     crs='EPSG:4326')\n",
    "\n",
    "\n",
    "        gdf.plot(ax=ax, alpha=0.2, markersize=5, c='b')\n",
    "\n",
    "        data_sub.total_bounds = [-8, 49,3,61]\n",
    "        geoplot.kdeplot(data_sub,shade=True, cmap='Reds',ax=ax)\n",
    "        plt.title('%s' % (pd.unique(data_sub['eventDate processed'])[0]))        \n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.savefig(path_root + 'heatmap/birdDist_%s_%d.png' % (year, day), dpi=100)\n",
    "    \n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have imported the data and have access to the spatial and timestamp data, let's look for annual trends in the data. To start, let's collapse sightings by month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "indent": "-40"
   },
   "outputs": [],
   "source": [
    "# Use pandas' crosstab function to count swallwo sightings by year x month\n",
    "data = pd.crosstab(columns=Swallows['month processed'], index=Swallows['year processed'],normalize=False)\n",
    "\n",
    "# We need to get the month names as column titles instead of 1~12. This is a quick way to do this\n",
    "months =  [ datetime.date(1900, x, 1).strftime('%B') for x in np.arange(1,13)]\n",
    "data.columns = months\n",
    "\n",
    "# Plot a  heatmap , with black/white blocks indicating months with less/more than 5 swallow sighings across the UK\n",
    "fig = plt.figure(figsize=(8, 12))\n",
    "hm  = sns.heatmap(data>5, annot=False,cbar=False)\n",
    "\n",
    "# Tidy the plot up\n",
    "plt.ylabel('Year of Records', fontsize=20)\n",
    "plt.xlabel('Month of Year', fontsize=20)\n",
    "plt.xticks(rotation=45, fontsize=15,ha='right')\n",
    "\n",
    "plt.title(\"\"\"Months with more than 5 recorded daily\n",
    "swallow sightings across the UK\"\"\", fontsize=20)\n",
    "\n",
    "# Save the figure as a .jpeg\n",
    "plt.savefig(path_root + '/plot/Year by Month.jpeg', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same process, but now narrowing into a weekly basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "indent": "-40"
   },
   "outputs": [],
   "source": [
    "# count occurrence by year-week\n",
    "data = pd.crosstab(columns=Swallows['week'], index=Swallows['year processed'],normalize=False)\n",
    "\n",
    "# heatmap\n",
    "fig = plt.figure(figsize=(9, 16))\n",
    "hm = sns.heatmap(data>5, annot=False,cbar=False)\n",
    "\n",
    "plt.ylabel('Year of Records', fontsize=20)\n",
    "plt.xlabel('Week of Year', fontsize=20)\n",
    "plt.xticks(rotation=45, fontsize=10,ha='right')\n",
    "plt.title(\"\"\"Weeks with at least five daily swallow sighting across the UK\"\"\", fontsize=20)\n",
    "\n",
    "plt.savefig(path_root + '/plot/Year by Week.jpeg', dpi=300)\n",
    "plt.xlim([0,51])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that as we move down the Y axis (forward in time), that the span of the white blocks is increasing: This suggests that the timerange of weeks over which swallows are sighted in the UK is increasing on an annual basis. \n",
    "\n",
    "in order to quantify this, we need an objective measure of arrival and departure dates. let's try to automatically extract the annaul arrival and departure dates, with a weekly resolution.\n",
    "- We'll threshold the data into yes/no sighings each week to get an array of Trues and Falses, one for each week. \n",
    "\n",
    "We then want to define motifs (i.e. temporal features) in the timeseries corresponding to consistent switches in observations that mark the arrival and departure periods.\n",
    "-  It's usually easiest and fastest to do this on binary data by converting it to strings, and using string find methods.\n",
    "\n",
    "We will search the weekly data to find the patterns. To do this, we'll assume the following abitrary motifs text to search for:\n",
    "- Arrival = 'fftttt' ( two weeks of absence followed by a sustained presence)\n",
    "- Departure = 'ttffff' (Two weeks of presence followed by a sustained absence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "indent": "-40"
   },
   "outputs": [],
   "source": [
    "# take the weekly sighting count across the UK for each year of observations\n",
    "data = pd.crosstab(columns=Swallows['week'], index=Swallows['year processed'],normalize=False)\n",
    "\n",
    "# Binarise the data: threshold by whether there were at least five sightings for each week of observation\n",
    "sightingMatrix = (data>5) \n",
    "\n",
    "# Optionally convert True/False to 1/0\n",
    "# sightingMatrix = sightingMatrix.astype('uint8') \n",
    "\n",
    "arrivalPattern = 'fftttt'\n",
    "departurePattern = 'ttffff'\n",
    "\n",
    "# Here's a quick function to collapse the list of Trues and Falses into strings of t's and f's:\n",
    "def bool_list_to_str(bool_list):\n",
    "    return ''.join('ft'[i] for i in bool_list)\n",
    "\n",
    "# Make a new dataframe to hold the detected week number for each of the arrival and departure times\n",
    "Migration = pd.DataFrame(columns = [\"Arrival\", \"Departure\"])\n",
    "\n",
    "# Now, apply our search function 'bool_list_to_str' to each year (row) of the binarised sighting matrix and catch the numbers into the relevant columns of the 'Migration' dataframe\n",
    "Migration['Arrival']   = sightingMatrix.apply(lambda x: bool_list_to_str(x).find(arrivalPattern),axis =1) \n",
    "Migration['Departure'] = sightingMatrix.apply(lambda x: bool_list_to_str(x).find(departurePattern),axis =1) \n",
    "\n",
    "# Years with no detected motifs are returned with -1 by default. Let's replace with NaNs:\n",
    "Migration = Migration.replace(-1, value=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a new Pandas dataframe called ```Migration```, containing the estimated week of the calendar year of arrival and departure for swallows across the UK. We can then use this to fit models make predicitions, and in the future, test for correlations and causal factors that might affect the animals' behaviour.\n",
    "\n",
    "Let's plot the estimated Arrival and Departrue data and look for annual trends in the detected migration timepoints. We will use ```SciKit-Learn```, a popular machine learning library to fit a cross-validated linear regression (resistant to over-fitting) and use the fitted trends to predict the annual changes: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "indent": "-20"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# We'll loop over arrival and departure events\n",
    "events  = ['Arrival','Departure']\n",
    "\n",
    "# Need to correct for the two week offset (in opposite directions) \n",
    "# This is where the detected jumps occur after the second week in the search templates above:\n",
    "offset = [2,-2]\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "\n",
    "# Let's use SciKit learn to fit a simple ordinary least squares linear regression to work out the rate of change per annum for each of the Arrival and Departure timeseries:\n",
    "ols=[linear_model.LinearRegression(),linear_model.LinearRegression()]\n",
    "\n",
    "# ... Or optionally try ridge regression for a more robust fit:\n",
    "# ols=[linear_model.Ridge(alpha=.1),linear_model.Ridge(alpha=.1)]\n",
    "\n",
    "for counter, event in enumerate(events):\n",
    "    # Grab some X and Y data to fit (years and detected week of change)\n",
    "    X = Migration.index.values\n",
    "    Y = Migration[event].values \n",
    "\n",
    "    # Trim off the years without detected arrival jumps  - the fitting can't handle the NaNs\n",
    "    idx = np.isnan(Y)\n",
    "    X = X[~idx].reshape(-1,1)\n",
    "    Y = Y[~idx].reshape(-1,1)\n",
    "    \n",
    "    # Correct the week data for the template offsets:\n",
    "    Y = Y + offset[counter] \n",
    "\n",
    "    # Split the datapoints into separate train/test sets for five-fold cross-validation \n",
    "    # (Really we only want the fit coeffs but it's good practice for estimating slopes):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "    \n",
    "    # fit the regression model:\n",
    "    ols[counter].fit(X_train,y_train)\n",
    "\n",
    "    # Return the slope:\n",
    "    # print(ols[counter].coef_) # In weekly change\n",
    "    # print(abs(ols[counter].coef_[0]*7))# In daily change\n",
    "    \n",
    "    # Now predict the values for the witheld test data points\n",
    "    y_pred = ols[counter].predict(X_test)\n",
    "    \n",
    "    # Get the slope of the regression line, convert from weekly to daily trend by multiplying by 7:\n",
    "    slope = abs(ols[counter].coef_[0]*7)\n",
    "\n",
    "    # Overlay the slope as text on the plot \n",
    "    axs[counter].text(1910,5,\"Rate of change =\") \n",
    "    axs[counter].text(1910,2,f'{slope[0]:.2f} days per year')\n",
    "\n",
    "    # Plot the data and trends\n",
    "    axs[counter].scatter(Migration.index.values, Migration[event].values+ offset[counter],  color='gray')\n",
    "    axs[counter].plot(X_test, y_pred, color='red', linewidth=2)\n",
    "    axs[counter].set_ylim([0,52]) \n",
    "    axs[counter].set_xlim([1900,2050]) \n",
    "        \n",
    "    axs[counter].set_xlabel('Year of records')\n",
    "    axs[counter].set_title('Annual ' + event.lower() + ' date')\n",
    "    if (counter ==0):\n",
    "        axs[counter].set_ylabel('Week of year')\n",
    "\n",
    "    del X,Y,idx, X_train, X_test, y_train, y_test\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(path_root + '/plot/Yearly trend.jpeg', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "632.841px",
    "left": "2674.55px",
    "right": "20px",
    "top": "120px",
    "width": "360px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
